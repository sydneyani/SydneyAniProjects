{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ded54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDS 6397 - Natural Language Processing\n",
    "# Assignment #5 - Sentence Pair Classification - Task 1: Paraphrase Detection\n",
    "\n",
    "# Install necessary packages (if not already installed)\n",
    "!pip install transformers datasets evaluate torch scikit-learn matplotlib\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the MRPC dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "print(dataset)\n",
    "\n",
    "# Examine a sample\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Define metrics computation function\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# PART 1: FINE-TUNING WITH FROZEN BASE MODEL\n",
    "# ==========================================\n",
    "\n",
    "# Load pre-trained model\n",
    "model_frozen = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Freeze all parameters except the classification head\n",
    "for param in model_frozen.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Count trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "frozen_params = count_parameters(model_frozen)\n",
    "print(f\"Number of trainable parameters (frozen base model): {frozen_params}\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args_frozen = TrainingArguments(\n",
    "    output_dir=\"./results_mrpc_frozen\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,  # Higher learning rate for fewer trainable parameters\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,  # Min 10, max 20 epochs as per instructions\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",  # Disable wandb\n",
    "    run_name=\"mrpc_frozen\"  # Explicitly set run name\n",
    ")\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_frozen = Trainer(\n",
    "    model=model_frozen,\n",
    "    args=training_args_frozen,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training frozen model...\")\n",
    "trainer_frozen.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results_frozen = trainer_frozen.evaluate()\n",
    "print(f\"Validation results (frozen model): {eval_results_frozen}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results_frozen = trainer_frozen.predict(test_dataset)\n",
    "print(f\"Test results (frozen model):\")\n",
    "print(f\"Accuracy: {test_results_frozen.metrics['test_accuracy']}\")\n",
    "\n",
    "# Get detailed metrics for each class\n",
    "test_preds = np.argmax(test_results_frozen.predictions, axis=1)\n",
    "test_labels = test_results_frozen.label_ids\n",
    "class_report = classification_report(test_labels, test_preds, target_names=[\"Not Paraphrase\", \"Paraphrase\"], output_dict=True)\n",
    "print(classification_report(test_labels, test_preds, target_names=[\"Not Paraphrase\", \"Paraphrase\"]))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "loss_values = [x.get('loss', 0) for x in trainer_frozen.state.log_history if 'loss' in x]\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Training Loss (Frozen)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "eval_results = [x.get('eval_accuracy', 0) for x in trainer_frozen.state.log_history if 'eval_accuracy' in x]\n",
    "plt.plot(eval_results)\n",
    "plt.title(\"Validation Accuracy (Frozen)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mrpc_frozen_training.png\")\n",
    "plt.show()\n",
    "\n",
    "# PART 2: FINE-TUNING WITH UNFROZEN MODEL\n",
    "# =======================================\n",
    "\n",
    "# Load pre-trained model again (all weights trainable)\n",
    "model_unfrozen = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "unfrozen_params = count_parameters(model_unfrozen)\n",
    "print(f\"Number of trainable parameters (unfrozen model): {unfrozen_params}\")\n",
    "\n",
    "# Define training arguments (smaller learning rate for all parameters)\n",
    "training_args_unfrozen = TrainingArguments(\n",
    "    output_dir=\"./results_mrpc_unfrozen\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,  # Lower learning rate when fine-tuning all parameters\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,  # Min 3, max 5 epochs as per instructions\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",  # Disable wandb\n",
    "    run_name=\"mrpc_unfrozen\"  # Explicitly set run name\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_unfrozen = Trainer(\n",
    "    model=model_unfrozen,\n",
    "    args=training_args_unfrozen,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training unfrozen model...\")\n",
    "trainer_unfrozen.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results_unfrozen = trainer_unfrozen.evaluate()\n",
    "print(f\"Validation results (unfrozen model): {eval_results_unfrozen}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results_unfrozen = trainer_unfrozen.predict(test_dataset)\n",
    "print(f\"Test results (unfrozen model):\")\n",
    "print(f\"Accuracy: {test_results_unfrozen.metrics['test_accuracy']}\")\n",
    "\n",
    "# Get detailed metrics for each class\n",
    "test_preds_unfrozen = np.argmax(test_results_unfrozen.predictions, axis=1)\n",
    "test_labels_unfrozen = test_results_unfrozen.label_ids\n",
    "class_report_unfrozen = classification_report(test_labels_unfrozen, test_preds_unfrozen, target_names=[\"Not Paraphrase\", \"Paraphrase\"], output_dict=True)\n",
    "print(classification_report(test_labels_unfrozen, test_preds_unfrozen, target_names=[\"Not Paraphrase\", \"Paraphrase\"]))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "loss_values = [x.get('loss', 0) for x in trainer_unfrozen.state.log_history if 'loss' in x]\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Training Loss (Unfrozen)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "eval_results = [x.get('eval_accuracy', 0) for x in trainer_unfrozen.state.log_history if 'eval_accuracy' in x]\n",
    "plt.plot(eval_results)\n",
    "plt.title(\"Validation Accuracy (Unfrozen)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mrpc_unfrozen_training.png\")\n",
    "plt.show()\n",
    "\n",
    "# COMPARE RESULTS\n",
    "frozen_metrics = {\n",
    "    \"accuracy\": test_results_frozen.metrics['test_accuracy'],\n",
    "    \"not_paraphrase_precision\": class_report[\"Not Paraphrase\"][\"precision\"],\n",
    "    \"not_paraphrase_recall\": class_report[\"Not Paraphrase\"][\"recall\"],\n",
    "    \"not_paraphrase_f1\": class_report[\"Not Paraphrase\"][\"f1-score\"],\n",
    "    \"paraphrase_precision\": class_report[\"Paraphrase\"][\"precision\"],\n",
    "    \"paraphrase_recall\": class_report[\"Paraphrase\"][\"recall\"],\n",
    "    \"paraphrase_f1\": class_report[\"Paraphrase\"][\"f1-score\"],\n",
    "}\n",
    "\n",
    "unfrozen_metrics = {\n",
    "    \"accuracy\": test_results_unfrozen.metrics['test_accuracy'],\n",
    "    \"not_paraphrase_precision\": class_report_unfrozen[\"Not Paraphrase\"][\"precision\"],\n",
    "    \"not_paraphrase_recall\": class_report_unfrozen[\"Not Paraphrase\"][\"recall\"],\n",
    "    \"not_paraphrase_f1\": class_report_unfrozen[\"Not Paraphrase\"][\"f1-score\"],\n",
    "    \"paraphrase_precision\": class_report_unfrozen[\"Paraphrase\"][\"precision\"],\n",
    "    \"paraphrase_recall\": class_report_unfrozen[\"Paraphrase\"][\"recall\"],\n",
    "    \"paraphrase_f1\": class_report_unfrozen[\"Paraphrase\"][\"f1-score\"],\n",
    "}\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"{'Metric':<25} {'Frozen Base':<15} {'Unfrozen':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for metric in frozen_metrics:\n",
    "    print(f\"{metric:<25} {frozen_metrics[metric]:<15.4f} {unfrozen_metrics[metric]:<15.4f}\")\n",
    "\n",
    "# Save models (optional)\n",
    "trainer_frozen.save_model(\"./final_mrpc_frozen\")\n",
    "trainer_unfrozen.save_model(\"./final_mrpc_unfrozen\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
